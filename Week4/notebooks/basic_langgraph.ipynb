{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install python-dotenv if not already installed\n",
        "%pip install python-dotenv langgraph>=0.6.4 langchain-openai>0.3.29\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required modules\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# You can check if required environment variables are set\n",
        "def validate_env_vars(required_vars):\n",
        "    missing_vars = []\n",
        "    for var in required_vars:\n",
        "        if os.getenv(var) is None:\n",
        "            missing_vars.append(var)\n",
        "    return missing_vars\n",
        "\n",
        "# List of required environment variables\n",
        "required_vars = ['OPENAI_API_KEY', 'OPENAI_MODEL']\n",
        "\n",
        "# Check for missing variables\n",
        "missing = validate_env_vars(required_vars)\n",
        "if missing:\n",
        "    print(f\"Warning: The following required environment variables are not set: {', '.join(missing)}\")\n",
        "else:\n",
        "    print(\"All required environment variables are set!\")\n",
        "\n",
        "# You can also load environment variables from a specific file\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# This will find the .env file automatically\n",
        "env_file = find_dotenv()\n",
        "print(f\"\\nFound .env file at: {env_file}\")\n",
        "\n",
        "# Reload environment variables (useful when .env file has changed)\n",
        "load_dotenv(override=True)  # override=True will reload all variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required typing utilities for type hints and annotations\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Import LangGraph components:\n",
        "# - StateGraph: Main class for building the conversation flow\n",
        "# - START: Special node marking the beginning of the graph\n",
        "# - END: Special node marking the end of the graph\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "# Import message handling utility\n",
        "#https://langchain-ai.github.io/langgraph/reference/graphs/#graph-definitions\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "\n",
        "# Define the structure of our conversation state\n",
        "class State(TypedDict):\n",
        "    \"\"\"\n",
        "    Defines the structure of the state that flows through our conversation graph.\n",
        "    \n",
        "    The state contains:\n",
        "    - messages: A list of conversation messages (both user and bot)\n",
        "    \n",
        "    The Annotated[list, add_messages] means:\n",
        "    - The field is a list\n",
        "    - When updating, use add_messages function to append new messages\n",
        "    - This prevents overwriting previous messages, maintaining conversation history\n",
        "    \"\"\"\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "# Initialize the graph builder with our State type\n",
        "# This creates a directed graph where:\n",
        "# - Nodes will be our processing steps (like the chatbot function)\n",
        "# - Edges define how messages flow between nodes\n",
        "# - The state (messages) flows through this graph structure\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# The graph will be used to:\n",
        "# 1. Take user input (START)\n",
        "# 2. Process it through our chatbot node\n",
        "# 3. Return the response (END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the LangChain function to initialize chat models\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Initialize the Large Language Model (LLM) for our chatbot\n",
        "# Parameters explained:\n",
        "# - \"openai:gpt-4.1\" specifies:\n",
        "#   - Provider: OpenAI\n",
        "#   - Model: GPT-4.1 (latest version)\n",
        "# - This model will be used to generate responses in our chat application\n",
        "# - The model requires an OPENAI_API_KEY to be set in the environment variables\n",
        "#   (which we verified in the previous cell)\n",
        "llm = init_chat_model(\"openai:gpt-4.1\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the chatbot node function that processes messages in the graph\n",
        "def chatbot(state: State):\n",
        "    \"\"\"\n",
        "    This function defines how the chatbot processes messages in the LangGraph.\n",
        "    \n",
        "    Args:\n",
        "        state (State): Contains the current conversation state with a 'messages' key\n",
        "                      holding the list of all messages so far\n",
        "    \n",
        "    Returns:\n",
        "        dict: Returns a new state with the bot's response added to messages\n",
        "    \"\"\"\n",
        "    # llm.invoke() takes the current messages and generates a response\n",
        "    # state[\"messages\"] contains the conversation history\n",
        "    # The response is wrapped in a list to maintain the message chain format\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Example of what this function does:\n",
        "# Input state:  {\"messages\": [HumanMessage(content=\"Hello!\")]}\n",
        "# llm.invoke() generates response like: AIMessage(content=\"Hi! How can I help?\")\n",
        "# Output state: {\"messages\": [AIMessage(content=\"Hi! How can I help?\")]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The first argument is the unique node name\n",
        "# The second argument is the function or object that will be called whenever\n",
        "# the node is used.\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of how to use the graph with user input\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Step 1: Start with empty state\n",
        "empty_state = {\n",
        "    \"messages\": []\n",
        "}\n",
        "print(\"1. Empty initial state:\", empty_state)\n",
        "\n",
        "# Step 2: First user message\n",
        "first_user_input = \"Hello, how are you?\"\n",
        "initial_state = {\n",
        "    \"messages\": [HumanMessage(content=first_user_input)]\n",
        "}\n",
        "print(\"\\n2. State after first user input:\", initial_state)\n",
        "\n",
        "# Step 3: Get bot's response to first message\n",
        "first_result = graph.invoke(initial_state)\n",
        "\n",
        "# Print the complete result to show the full state structure\n",
        "# This includes:\n",
        "# - The original user message (with metadata like ID)\n",
        "# - The bot's response (with metadata like token usage, model info)\n",
        "# - Additional metadata about the conversation\n",
        "print(\"\\nComplete state after bot response:\")\n",
        "print(\"- Contains both user and bot messages\")\n",
        "print(\"- Includes conversation metadata\")\n",
        "print(\"Raw state:\", first_result)\n",
        "\n",
        "# Print just the bot's response in a user-friendly format\n",
        "# Breaking down first_result[\"messages\"][-1].content:\n",
        "# 1. first_result[\"messages\"] - Gets the list of all messages\n",
        "# 2. [-1] - Gets the last message in the list (negative index counts from end)\n",
        "# 3. .content - Gets just the text content of that message\n",
        "#\n",
        "# Example structure:\n",
        "# first_result = {\n",
        "#     \"messages\": [\n",
        "#         HumanMessage(content=\"Hello, how are you?\"),  # index 0\n",
        "#         AIMessage(content=\"Hi! How can I help?\")      # index -1 (last)\n",
        "#     ]\n",
        "# }\n",
        "print(\"\\n3. First bot response:\", first_result[\"messages\"][-1].content)\n",
        "\n",
        "# Let's also print each part to see how we get to the content:\n",
        "print(\"\\nBreaking down the steps:\")\n",
        "print(\"1. All messages:\", first_result[\"messages\"])  # Shows full message list\n",
        "print(\"2. Last message:\", first_result[\"messages\"][-1])  # Shows last message with metadata\n",
        "print(\"3. Just content:\", first_result[\"messages\"][-1].content)  # Shows just the text\n",
        "\n",
        "# Step 4: Add second user message\n",
        "# Note: We need to include the previous messages to maintain conversation history\n",
        "second_user_input = \"What can you help me with?\"\n",
        "second_state = {\n",
        "    \"messages\": first_result[\"messages\"] + [HumanMessage(content=second_user_input)]\n",
        "}\n",
        "print(\"\\n4. State after adding second user message:\")\n",
        "print(\"- Previous bot response remains in history\")\n",
        "print(\"- New user message added\")\n",
        "for idx, msg in enumerate(second_state[\"messages\"]):\n",
        "    print(f\"Message {idx + 1}: {'User' if isinstance(msg, HumanMessage) else 'Bot'} - {msg.content}\")\n",
        "\n",
        "# Step 5: Get bot's response to second message\n",
        "second_result = graph.invoke(second_state)\n",
        "print(\"\\n5. Second bot response:\", second_result[\"messages\"][-1].content)\n",
        "\n",
        "# Step 6: Show final conversation state\n",
        "print(\"\\n6. Final conversation state:\")\n",
        "for idx, msg in enumerate(second_result[\"messages\"]):\n",
        "    print(f\"Message {idx + 1}: {'User' if isinstance(msg, HumanMessage) else 'Bot'} - {msg.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic LangGraph Chatbot\n",
        "# CONVERSATION WITH A BOT\n",
        "# Maintains conversation in memory\n",
        "# Import ipywidgets for interactive input\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import display, clear_output\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Create widgets for the chat interface\n",
        "text_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type your message here...',\n",
        "    description='Input:',\n",
        "    layout={'width': '500px'}\n",
        ")\n",
        "send_button = widgets.Button(\n",
        "    description='Send',\n",
        "    button_style='primary',\n",
        "    layout={'width': '100px'}\n",
        ")\n",
        "output = widgets.Output()\n",
        "\n",
        "messages = []\n",
        "\n",
        "def on_send_button_clicked(b):\n",
        "    with output:\n",
        "        # clear_output()\n",
        "        # Get the input text\n",
        "        user_input = text_input.value\n",
        "        if not user_input.strip():  # Skip empty messages\n",
        "            return\n",
        "            \n",
        "        # Clear the input field\n",
        "        text_input.value = ''\n",
        "        \n",
        "        # Add user message to the state\n",
        "        messages.append(HumanMessage(content=user_input))\n",
        "        \n",
        "        # Display user message\n",
        "        print(f\"You: {user_input}\")\n",
        "        \n",
        "        # Run the graph\n",
        "        result = graph.invoke({\"messages\": messages})\n",
        "        \n",
        "        # Get the latest response and add it to messages\n",
        "        bot_response = result[\"messages\"][-1].content\n",
        "        messages.append(AIMessage(content=bot_response))\n",
        "        \n",
        "        # Display bot response\n",
        "        print(f\"\\nBot: {bot_response}\")\n",
        "\n",
        "# Connect the button click to the handler\n",
        "send_button.on_click(on_send_button_clicked)\n",
        "\n",
        "# Display the chat interface\n",
        "display(widgets.VBox([\n",
        "    widgets.HBox([text_input, send_button]),\n",
        "    output\n",
        "]))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "code4training",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
