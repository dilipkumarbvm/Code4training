{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Support Workflow with LangGraph\n",
        "\n",
        "This notebook demonstrates a practical customer support workflow using LangGraph with two nodes:\n",
        "1. Query Analyzer Node: Analyzes customer queries for sentiment and urgency\n",
        "2. Response Generator Node: Generates appropriate responses based on the analysis\n",
        "\n",
        "We'll use this to handle customer support queries in a more intelligent way by first understanding the context and then crafting appropriate responses.\n",
        "\n",
        "# Install required packages if not already installed\n",
        "%pip install python-dotenv langgraph>=0.6.4 langchain-openai>0.3.29\n",
        "\n",
        "# Restart the kernel after installing packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "from typing import Annotated, Sequence, Optional\n",
        "from typing_extensions import TypedDict\n",
        "from datetime import datetime\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
        "\n",
        "# Load environment variables\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # This will load your OPENAI_API_KEY from .env file\n",
        "\n",
        "# Initialize the LLM with environment variables\n",
        "llm = init_chat_model(\"openai:gpt-4.1-mini\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define our state type\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], \"The messages in the conversation\"]\n",
        "    analysis: Annotated[dict, \"Analysis of the customer query\"]\n",
        "    response_needed: Annotated[bool, \"Whether the query requires a response\"]\n",
        "    filter_reason: Annotated[Optional[str], \"Reason for not responding if response not needed\"]\n",
        "\n",
        "# Query Analyzer Node\n",
        "def analyze_query(state: State) -> State:\n",
        "    # Get the last message (customer query)\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    \n",
        "    # Analyze the query using the LLM\n",
        "    analysis_prompt = f\"\"\"\n",
        "    Analyze the following customer support query for:\n",
        "    1. Sentiment (positive, negative, neutral)\n",
        "    2. Urgency (high, medium, low)\n",
        "    3. Category (billing, technical, account, general)\n",
        "    4. Key points (max 3)\n",
        "\n",
        "    Query: {last_message.content}\n",
        "\n",
        "    Provide the analysis in a JSON format with these exact keys:\n",
        "    sentiment, urgency, category, key_points\n",
        "    \"\"\"\n",
        "    \n",
        "    analysis_response = llm.invoke(analysis_prompt)\n",
        "    \n",
        "    # Update state with analysis using json.loads instead of eval\n",
        "    import json\n",
        "    state[\"analysis\"] = json.loads(analysis_response.content)  # Convert string to dict\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter Node\n",
        "def filter_query(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Analyzes the query to determine if it requires a response.\n",
        "    \"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    \n",
        "    filter_prompt = f\"\"\"\n",
        "    Analyze the following customer query and determine if it requires a response.\n",
        "    Consider the following criteria:\n",
        "    1. Is it a genuine question or request?\n",
        "    2. Is it appropriate and business-related?\n",
        "    3. Does it contain enough context to be actionable?\n",
        "    4. Is it not spam or automated content?\n",
        "    \n",
        "    Query: {last_message.content}\n",
        "    \n",
        "    Respond in JSON format with these exact keys:\n",
        "    - needs_response: boolean (true/false)\n",
        "    - reason: string (explanation if no response needed)\n",
        "    \"\"\"\n",
        "    \n",
        "    filter_response = llm.invoke(filter_prompt)\n",
        "    \n",
        "    # Use json.loads instead of eval for safer JSON parsing\n",
        "    import json\n",
        "    filter_result = json.loads(filter_response.content)\n",
        "    \n",
        "    # Update state with filter decision\n",
        "    state[\"response_needed\"] = filter_result[\"needs_response\"]\n",
        "    state[\"filter_reason\"] = filter_result.get(\"reason\") if not filter_result[\"needs_response\"] else None\n",
        "    \n",
        "    return state\n",
        "\n",
        "# Define conditional routing\n",
        "def should_respond(state: State) -> str:\n",
        "    \"\"\"\n",
        "    Routes to either the response generator or end based on filter decision.\n",
        "    \"\"\"\n",
        "    return \"responder\" if state[\"response_needed\"] else END\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Response Generator Node\n",
        "def generate_response(state: State) -> State:\n",
        "    # Get the analysis and customer query\n",
        "    analysis = state[\"analysis\"]\n",
        "    customer_query = state[\"messages\"][-1].content\n",
        "    \n",
        "    # Create a prompt that uses the analysis to generate an appropriate response\n",
        "    response_prompt = f\"\"\"\n",
        "    You are a customer support agent. Generate a response to the customer query below.\n",
        "    Use the provided analysis to tailor your response appropriately.\n",
        "\n",
        "    Customer Query: {customer_query}\n",
        "\n",
        "    Analysis:\n",
        "    - Sentiment: {analysis['sentiment']}\n",
        "    - Urgency: {analysis['urgency']}\n",
        "    - Category: {analysis['category']}\n",
        "    - Key Points: {', '.join(analysis['key_points'])}\n",
        "\n",
        "    Guidelines:\n",
        "    - For high urgency, express immediate attention\n",
        "    - For negative sentiment, show extra empathy\n",
        "    - For technical issues, be clear and step-by-step\n",
        "    - For billing issues, be precise about numbers/dates\n",
        "    - Keep the tone professional but friendly\n",
        "    - Be concise but thorough\n",
        "\n",
        "    Generate the response:\n",
        "    \"\"\"\n",
        "    \n",
        "    response = llm.invoke(response_prompt)\n",
        "    \n",
        "    # Add the response to the messages\n",
        "    state[\"messages\"].append(response)\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the workflow graph\n",
        "graph = (\n",
        "    StateGraph(State)\n",
        "    .add_node(\"analyzer\", analyze_query)\n",
        "    .add_node(\"filter\", filter_query)\n",
        "    .add_node(\"responder\", generate_response)\n",
        "    .set_entry_point(\"analyzer\")\n",
        "    .add_edge(\"analyzer\", \"filter\")\n",
        "    .add_conditional_edges(\n",
        "        \"filter\",\n",
        "        should_respond,\n",
        "        {\n",
        "            \"responder\": \"responder\",\n",
        "            END: END\n",
        "        }\n",
        "    )\n",
        "    .add_edge(\"responder\", END)\n",
        "    .compile()\n",
        ")\n",
        "\n",
        "def process_customer_query(query: str) -> State:\n",
        "    \"\"\"\n",
        "    Process a customer query through our workflow\n",
        "    \"\"\"\n",
        "    # Initialize the state with the customer's message\n",
        "    initial_state = State(\n",
        "        messages=[HumanMessage(content=query)],\n",
        "        analysis={},\n",
        "        response_needed=True,  # Default to True, will be updated by filter\n",
        "        filter_reason=None\n",
        "    )\n",
        "    \n",
        "    # Run the workflow\n",
        "    result = graph.invoke(initial_state)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mermaid Syntax:\n",
            "---\n",
            "config:\n",
            "  flowchart:\n",
            "    curve: linear\n",
            "---\n",
            "graph TD;\n",
            "\t__start__([<p>__start__</p>]):::first\n",
            "\tanalyzer(analyzer)\n",
            "\tfilter(filter)\n",
            "\tresponder(responder)\n",
            "\t__end__([<p>__end__</p>]):::last\n",
            "\t__start__ --> analyzer;\n",
            "\tanalyzer --> filter;\n",
            "\tfilter -.-> __end__;\n",
            "\tfilter -.-> responder;\n",
            "\tresponder --> __end__;\n",
            "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
            "\tclassDef first fill-opacity:0\n",
            "\tclassDef last fill:#bfb6fc\n",
            "\n",
            "\n",
            "Graph Visualization:\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize the graph structure using LangGraph's built-in visualization\n",
        "from IPython.display import Image, display\n",
        "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
        "\n",
        "# Get the Mermaid syntax\n",
        "print(\"Mermaid Syntax:\")\n",
        "print(graph.get_graph().draw_mermaid())\n",
        "\n",
        "# Display the graph as PNG\n",
        "print(\"\\nGraph Visualization:\")\n",
        "display(Image(graph.get_graph().draw_mermaid_png(\n",
        "    curve_style=CurveStyle.LINEAR,\n",
        "    node_colors=NodeStyles(\n",
        "        first=\"#90EE90\",  # Light green for start\n",
        "        last=\"#FFB6C1\",   # Light pink for end\n",
        "        default=\"#ADD8E6\"  # Light blue for other nodes\n",
        "    ),\n",
        "    wrap_label_n_words=9,\n",
        "    background_color=\"white\",\n",
        "    padding=10\n",
        ")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Customer Query: I've been charged twice for my subscription this month and need this fixed immediately!\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 1 (char 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCustomer Query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m result \u001b[38;5;241m=\u001b[39m process_customer_query(query)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuery Analysis:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manalysis\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[5], line 34\u001b[0m, in \u001b[0;36mprocess_customer_query\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     26\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m State(\n\u001b[0;32m     27\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[HumanMessage(content\u001b[38;5;241m=\u001b[39mquery)],\n\u001b[0;32m     28\u001b[0m     analysis\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m     29\u001b[0m     response_needed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Default to True, will be updated by filter\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     filter_reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Run the workflow\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m result \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39minvoke(initial_state)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\main.py:3019\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, **kwargs)\u001b[0m\n\u001b[0;32m   3016\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3017\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 3019\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   3020\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3021\u001b[0m     config,\n\u001b[0;32m   3022\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m   3023\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   3024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3025\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[0;32m   3026\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[0;32m   3027\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   3028\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   3029\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   3030\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3031\u001b[0m ):\n\u001b[0;32m   3032\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3033\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\main.py:2651\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2649\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2650\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2651\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2652\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[0;32m   2653\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2654\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2655\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[0;32m   2656\u001b[0m ):\n\u001b[0;32m   2657\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2658\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[0;32m   2659\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[0;32m   2660\u001b[0m     )\n\u001b[0;32m   2661\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
            "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     run_with_retry(\n\u001b[0;32m    163\u001b[0m         t,\n\u001b[0;32m    164\u001b[0m         retry_policy,\n\u001b[0;32m    165\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    166\u001b[0m             CONFIG_KEY_CALL: partial(\n\u001b[0;32m    167\u001b[0m                 _call,\n\u001b[0;32m    168\u001b[0m                 weakref\u001b[38;5;241m.\u001b[39mref(t),\n\u001b[0;32m    169\u001b[0m                 retry_policy\u001b[38;5;241m=\u001b[39mretry_policy,\n\u001b[0;32m    170\u001b[0m                 futures\u001b[38;5;241m=\u001b[39mweakref\u001b[38;5;241m.\u001b[39mref(futures),\n\u001b[0;32m    171\u001b[0m                 schedule_task\u001b[38;5;241m=\u001b[39mschedule_task,\n\u001b[0;32m    172\u001b[0m                 submit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit,\n\u001b[0;32m    173\u001b[0m             ),\n\u001b[0;32m    174\u001b[0m         },\n\u001b[0;32m    175\u001b[0m     )\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
            "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:646\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 646\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:390\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
            "Cell \u001b[1;32mIn[2], line 31\u001b[0m, in \u001b[0;36manalyze_query\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Update state with analysis using json.loads instead of eval\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manalysis\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(analysis_response\u001b[38;5;241m.\u001b[39mcontent)  \u001b[38;5;66;03m# Convert string to dict\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
            "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
            "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
            "File \u001b[1;32mc:\\Users\\kumar\\anaconda3\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
            "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
            "\u001b[0mDuring task with name 'analyzer' and id 'd2599274-f793-c0cb-8e5f-1703f0086b09'"
          ]
        }
      ],
      "source": [
        "# Example customer queries\n",
        "test_queries = [\n",
        "    \"I've been charged twice for my subscription this month and need this fixed immediately!\",\n",
        "    \"Could you help me understand how to export my data from the platform?\",\n",
        "    \"Just wanted to say thank you for the excellent service last week!\",\n",
        "    \"Buy cheap watches at www.spam.com!!!\",  # Spam message\n",
        "    \"\",  # Empty message\n",
        "    \"asdfghjkl\"  # Random keystrokes\n",
        "]\n",
        "\n",
        "# Process each query and display results\n",
        "for query in test_queries:\n",
        "    print(f\"\\nCustomer Query: {query}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    result = process_customer_query(query)\n",
        "    \n",
        "    print(\"\\nQuery Analysis:\")\n",
        "    print(f\"Sentiment: {result['analysis']['sentiment']}\")\n",
        "    print(f\"Urgency: {result['analysis']['urgency']}\")\n",
        "    print(f\"Category: {result['analysis']['category']}\")\n",
        "    print(f\"Key Points: {', '.join(result['analysis']['key_points'])}\")\n",
        "    \n",
        "    print(\"\\nFilter Decision:\")\n",
        "    print(f\"Response Needed: {result['response_needed']}\")\n",
        "    if not result['response_needed']:\n",
        "        print(f\"Filter Reason: {result['filter_reason']}\")\n",
        "    elif len(result['messages']) > 1:  # Check if we have a response\n",
        "        print(\"\\nGenerated Response:\")\n",
        "        print(result[\"messages\"][-1].content)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
