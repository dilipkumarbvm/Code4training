{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Support Workflow with LangGraph\n",
        "\n",
        "This notebook demonstrates a practical customer support workflow using LangGraph with two nodes:\n",
        "1. Query Analyzer Node: Analyzes customer queries for sentiment and urgency\n",
        "2. Response Generator Node: Generates appropriate responses based on the analysis\n",
        "\n",
        "We'll use this to handle customer support queries in a more intelligent way by first understanding the context and then crafting appropriate responses.\n",
        "\n",
        "# Install required packages if not already installed\n",
        "%pip install python-dotenv langgraph>=0.6.4 langchain-openai>0.3.29\n",
        "\n",
        "# Restart the kernel after installing packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "from typing import Annotated, Sequence, Optional\n",
        "from typing_extensions import TypedDict\n",
        "from datetime import datetime\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
        "\n",
        "# Load environment variables\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()  # This will load your OPENAI_API_KEY from .env file\n",
        "\n",
        "# Initialize the LLM with environment variables\n",
        "llm = init_chat_model(\"openai:gpt-4.1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define our state type\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], \"The messages in the conversation\"]\n",
        "    analysis: Annotated[dict, \"Analysis of the customer query\"]\n",
        "    response_needed: Annotated[bool, \"Whether the query requires a response\"]\n",
        "    filter_reason: Annotated[Optional[str], \"Reason for not responding if response not needed\"]\n",
        "\n",
        "# Query Analyzer Node\n",
        "def analyze_query(state: State) -> State:\n",
        "    # Get the last message (customer query)\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    \n",
        "    # Analyze the query using the LLM\n",
        "    analysis_prompt = f\"\"\"\n",
        "    Analyze the following customer support query for:\n",
        "    1. Sentiment (positive, negative, neutral)\n",
        "    2. Urgency (high, medium, low)\n",
        "    3. Category (billing, technical, account, general)\n",
        "    4. Key points (max 3)\n",
        "\n",
        "    Query: {last_message.content}\n",
        "\n",
        "    Provide the analysis in a JSON format with these exact keys:\n",
        "    sentiment, urgency, category, key_points\n",
        "    \"\"\"\n",
        "    \n",
        "    analysis_response = llm.invoke(analysis_prompt)\n",
        "    \n",
        "    # Update state with analysis\n",
        "    state[\"analysis\"] = eval(analysis_response.content)  # Convert string to dict\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter Node\n",
        "def filter_query(state: State) -> State:\n",
        "    \"\"\"\n",
        "    Analyzes the query to determine if it requires a response.\n",
        "    \"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    \n",
        "    filter_prompt = f\"\"\"\n",
        "    Analyze the following customer query and determine if it requires a response.\n",
        "    Consider the following criteria:\n",
        "    1. Is it a genuine question or request?\n",
        "    2. Is it appropriate and business-related?\n",
        "    3. Does it contain enough context to be actionable?\n",
        "    4. Is it not spam or automated content?\n",
        "    \n",
        "    Query: {last_message.content}\n",
        "    \n",
        "    Respond in JSON format with these exact keys:\n",
        "    - needs_response: boolean (true/false)\n",
        "    - reason: string (explanation if no response needed)\n",
        "    \"\"\"\n",
        "    \n",
        "    filter_response = llm.invoke(filter_prompt)\n",
        "    filter_result = eval(filter_response.content)\n",
        "    \n",
        "    # Update state with filter decision\n",
        "    state[\"response_needed\"] = filter_result[\"needs_response\"]\n",
        "    state[\"filter_reason\"] = filter_result.get(\"reason\") if not filter_result[\"needs_response\"] else None\n",
        "    \n",
        "    return state\n",
        "\n",
        "# Define conditional routing\n",
        "def should_respond(state: State) -> str:\n",
        "    \"\"\"\n",
        "    Routes to either the response generator or end based on filter decision.\n",
        "    \"\"\"\n",
        "    return \"responder\" if state[\"response_needed\"] else END\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Response Generator Node\n",
        "def generate_response(state: State) -> State:\n",
        "    # Get the analysis and customer query\n",
        "    analysis = state[\"analysis\"]\n",
        "    customer_query = state[\"messages\"][-1].content\n",
        "    \n",
        "    # Create a prompt that uses the analysis to generate an appropriate response\n",
        "    response_prompt = f\"\"\"\n",
        "    You are a customer support agent. Generate a response to the customer query below.\n",
        "    Use the provided analysis to tailor your response appropriately.\n",
        "\n",
        "    Customer Query: {customer_query}\n",
        "\n",
        "    Analysis:\n",
        "    - Sentiment: {analysis['sentiment']}\n",
        "    - Urgency: {analysis['urgency']}\n",
        "    - Category: {analysis['category']}\n",
        "    - Key Points: {', '.join(analysis['key_points'])}\n",
        "\n",
        "    Guidelines:\n",
        "    - For high urgency, express immediate attention\n",
        "    - For negative sentiment, show extra empathy\n",
        "    - For technical issues, be clear and step-by-step\n",
        "    - For billing issues, be precise about numbers/dates\n",
        "    - Keep the tone professional but friendly\n",
        "    - Be concise but thorough\n",
        "\n",
        "    Generate the response:\n",
        "    \"\"\"\n",
        "    \n",
        "    response = llm.invoke(response_prompt)\n",
        "    \n",
        "    # Add the response to the messages\n",
        "    state[\"messages\"].append(response)\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the workflow graph\n",
        "graph = (\n",
        "    StateGraph(State)\n",
        "    .add_node(\"analyzer\", analyze_query)\n",
        "    .add_node(\"filter\", filter_query)\n",
        "    .add_node(\"responder\", generate_response)\n",
        "    .set_entry_point(\"analyzer\")\n",
        "    .add_edge(\"analyzer\", \"filter\")\n",
        "    .add_conditional_edges(\n",
        "        \"filter\",\n",
        "        should_respond,\n",
        "        {\n",
        "            \"responder\": \"responder\",\n",
        "            END: END\n",
        "        }\n",
        "    )\n",
        "    .add_edge(\"responder\", END)\n",
        "    .compile()\n",
        ")\n",
        "\n",
        "def process_customer_query(query: str) -> State:\n",
        "    \"\"\"\n",
        "    Process a customer query through our workflow\n",
        "    \"\"\"\n",
        "    # Initialize the state with the customer's message\n",
        "    initial_state = State(\n",
        "        messages=[HumanMessage(content=query)],\n",
        "        analysis={},\n",
        "        response_needed=True,  # Default to True, will be updated by filter\n",
        "        filter_reason=None\n",
        "    )\n",
        "    \n",
        "    # Run the workflow\n",
        "    result = graph.invoke(initial_state)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the graph structure using LangGraph's built-in visualization\n",
        "from IPython.display import Image, display\n",
        "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
        "\n",
        "# Get the Mermaid syntax\n",
        "print(\"Mermaid Syntax:\")\n",
        "print(graph.get_graph().draw_mermaid())\n",
        "\n",
        "# Display the graph as PNG\n",
        "print(\"\\nGraph Visualization:\")\n",
        "display(Image(graph.get_graph().draw_mermaid_png(\n",
        "    curve_style=CurveStyle.LINEAR,\n",
        "    node_colors=NodeStyles(\n",
        "        first=\"#90EE90\",  # Light green for start\n",
        "        last=\"#FFB6C1\",   # Light pink for end\n",
        "        default=\"#ADD8E6\"  # Light blue for other nodes\n",
        "    ),\n",
        "    wrap_label_n_words=9,\n",
        "    background_color=\"white\",\n",
        "    padding=10\n",
        ")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example customer queries\n",
        "test_queries = [\n",
        "    \"I've been charged twice for my subscription this month and need this fixed immediately!\",\n",
        "    \"Could you help me understand how to export my data from the platform?\",\n",
        "    \"Just wanted to say thank you for the excellent service last week!\",\n",
        "    \"Buy cheap watches at www.spam.com!!!\",  # Spam message\n",
        "    \"\",  # Empty message\n",
        "    \"asdfghjkl\"  # Random keystrokes\n",
        "]\n",
        "\n",
        "# Process each query and display results\n",
        "for query in test_queries:\n",
        "    print(f\"\\nCustomer Query: {query}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    result = process_customer_query(query)\n",
        "    \n",
        "    print(\"\\nQuery Analysis:\")\n",
        "    print(f\"Sentiment: {result['analysis']['sentiment']}\")\n",
        "    print(f\"Urgency: {result['analysis']['urgency']}\")\n",
        "    print(f\"Category: {result['analysis']['category']}\")\n",
        "    print(f\"Key Points: {', '.join(result['analysis']['key_points'])}\")\n",
        "    \n",
        "    print(\"\\nFilter Decision:\")\n",
        "    print(f\"Response Needed: {result['response_needed']}\")\n",
        "    if not result['response_needed']:\n",
        "        print(f\"Filter Reason: {result['filter_reason']}\")\n",
        "    elif len(result['messages']) > 1:  # Check if we have a response\n",
        "        print(\"\\nGenerated Response:\")\n",
        "        print(result[\"messages\"][-1].content)\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "code4training",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
